%  SET UP LaTeX DEFAULTS  --------------------------------------
\documentclass{article}
\usepackage[sc]{mathpazo}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{3}
% \usepackage[options]{algorithm2e}
\usepackage{url}
\usepackage{setspace}
\usepackage{relsize}
\usepackage{float}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{booktabs}

\usepackage[authoryear]{natbib}
\usepackage[nottoc]{tocbibind}
\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black]{hyperref}
\hypersetup{
    pdfstartview={XYZ null null 1}}
%\usepackage{breakurl}
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[buttonsize=1em]{animate}
\makeatother
\renewcommand{\bibname}{References}

\begin{document}

\title{Air Pollution Exploratory Models}
\author{Robert A. Giaquinto - Master's Project}
\maketitle
\tableofcontents

%  Base KnitR code ------------------------------------------------------------------
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

opts_chunk$set(fig.align='center',
    fig.show='hold',
    fig.pos='H',
    message=FALSE,
    warning=FALSE,
    echo=TRUE,
    par=TRUE)
options(width=80, stringsAsFactors=FALSE)

library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(xtable)
library(glmnet)
library(stringr)
library(readr)
library(glmnetUtils)
library(glasso)
library(scout)
library(lubridate)
library(RColorBrewer)
library(ggmap)
source('validation_helpers.R')
source('Lasso.R')
source('Graphical_Lasso.R')
@


\section{Executive Summary}
This is what I found


\section{Data}
The data are generated by the \texttt{build\_features.py} file. The data are solely from the \textbf{Houston} area (i.e. training set) and comprise all available features for hourly files from the EPA's air pollution website\footnote{\url{http://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html}} between 2005 and 2015.


<<data, results='hide', cache=TRUE>>=

data_dir <- '/Users/robert/documents/umn/air_pollution/data/'

# read in a few rows of data to determine column types
temp <- read.csv(file=paste0(data_dir, "houston_features.csv"), nrow=1000)

# if column types are logical (from poor guessing of type) change to double
col_types <- sapply(temp, class)
col_types <- ifelse(col_types %in% c('logical', 'numeric'), 'd', str_sub(col_types,1,1))
col_types <- paste(col_types, collapse='')

DF <- read_csv(file=paste0(data_dir, "houston_features.csv"),
    progress=FALSE,
    col_types=col_types)
# remove temporary items
rm(temp, col_types)

# to speed up exploration, only use data from 2010 forward
full_locations <- c(167001034L, 201000024L, 201001034L,
    201001035L, 201001039L, 201001042L, 201001050L, 245000022L, 245001050L)
DF <- as.data.frame(DF[DF$date_key >= 20130000 & DF$location_key %in% full_locations,])
DF <- as.data.frame(DF[DF$pm25 < 100,])
# DF$pm25 <- ifelse(DF$pm25 == 0, NA, DF$pm25)
# DF <- DF[complete.cases(DF),]
DF$County_Code <- NULL
DF$State_Code <- NULL

# Create a few custom variables
DF$am_rushhour <- ifelse(DF$Time_Local %in% c(6,7,8), 1, 0)
DF$pm_rushhour <- ifelse(DF$Time_Local %in% c(18,19,20,21), 1, 0)
DF$rushhour <- ifelse(DF$am_rushhour > 0 | DF$pm_rushhour > 0, 1, 0)
@

<<var_names>>=
# define variables to refer to groups of variables
tar_var <- 'pm25'

key_vars <- c("date_key", "datetime_key", "location_key"
#     , "Date_Local"
    , "Latitude"
    , "Longitude"
#     , "Site_Num"
    , "Time_Local"
)

date_vars <- c("day_of_week_cycle",
    "day_of_week_0", "day_of_week_1", "day_of_week_2",
    "day_of_week_3", "day_of_week_4", "day_of_week_5", "day_of_week_6",
    "day_of_year_cycle", "week_of_year_cycle",
    "month_cycle", "month_1",
    "month_2", "month_3", "month_4", "month_5", "month_6", "month_7",
    "month_8", "month_9", "month_10", "month_11", "month_12",
    "day_of_month_cycle",
    "quarter_cycle", "quarter_1", "quarter_2", "quarter_3", "quarter_4",
    "time_cycle",
    "am_rushhour", "pm_rushhour", "rushhour"
)

lag_root_names <- c("pm25_lag", "wind_direction_sin_lag", "wind_direction_cos_lag",
    # "wind_direction_NE_lag", "wind_direction_SE_lag",
    # "wind_direction_NW_lag", "wind_direction_SW_lag",
    "wind_knots_lag", "temperature_lag",
    "pct_humidity_lag", "dewpoint_lag",
    "ozone_lag", "so2_lag", "co_lag", "no2_lag")

lag_vars <- c(paste0(lag_root_names, "1")
#     , paste0(lag_root_names, "2")
#     , paste0(lag_root_names, "3")
)

agg_vars <- c(paste0(lag_root_names, "1", "_agg1")
#     , paste0(lag_root_names, "2", "_agg1")
#     , paste0(lag_root_names, "3", "_agg1")
)

# keep only the variables used above for modeling
DF <- DF[,c(key_vars, tar_var, date_vars, lag_vars, agg_vars)]

# impute remainder of missings
for (n in names(DF)) {
    if (n %in% c(lag_vars, agg_vars)) {
        DF[is.na(DF[,n]), n] <- mean(DF[,n], na.rm=TRUE)
    }
}
@




There are 270 total features, many have been derived by lagging or lagging and aggregating the basic features. Additionally, there are binary features that capture mean-shifts related to days of the week, months, quarters, and days of the month.

<<feature_names, results='asis'>>=

# print names of the columns
all_names <- names(DF)
uniq_names <- unique(
    str_replace_all(
            str_replace_all(all_names[all_names %in% c(lag_vars, agg_vars, date_vars)],
                '(month_|quarter_|day_of_week_|day_of_month_|Time_Local_)([x0-9]+)', '\\1X'),
            '(lag)([_a-z0-9]+)', '\\1')
    )
if ((length(uniq_names) %% 2) > 0) {
    uniq_names <- c(uniq_names, "")
}
name_df <- data.frame(x1 = uniq_names[1:(length(uniq_names)/2)],
    x2 = uniq_names[(1 + length(uniq_names)/2):length(uniq_names)])
names(name_df) <- c("Features", "Additional Features")
print(xtable(name_df), include.rownames=FALSE, booktabs=TRUE)
@


Finally, the same date features have to converted into a continuous and cyclic number (e.g. Sundays and Mondays are close together)

<<cycles, results='asis'>>=
vars <- c("day_of_week_0",
    "day_of_week_1",
    "day_of_week_2",
    "day_of_week_3",
    "day_of_week_4",
    "day_of_week_5",
    "day_of_week_6",
    "day_of_week_cycle")
example <- as.data.frame(unique(DF[,vars]))
# order results
example <- example[order(example$day_of_week_cycle),]
#print to table
print(xtable(example, digits=0), include.rownames=FALSE, scalebox=.75)
@


<<cleanup_data, echo=FALSE, results='hide'>>=
rm(example, name_df, uniq_names, vars)
@
A full description of all the features can be found in the Appendix.

\subsection{Missing Data}
Missing data were replaced with averages from all other stations at the same point in time (i.e. the same hour). If there was still missing data, then the missing values were replaced with averages from all other stations on that same day. This appoach is reasonable since many of the missing values come from weather related features. It's reasonable to assume that within the same metropolitan area the temperature, wind speed and direction, etc. will be fairly similar. Moreover, missing data is largely the result of not all monitoring stations being equiped with the necessary instruments to capture all weather related features -- thus, missing data is not correlated to certain events that impact the recording of data.


\section{Evalutation}
This is how I split the data into testing and training.

Since the goal is predictive accuracy I evaluate the models based on prediction accuracy on the hold-out data set.


\section{Models}

<<training_parameters>>=
testing_months=6
training_months=12
verbose=FALSE
num_lambdas <- 10
var_list <- list(tar_var=tar_var, date_vars=date_vars, agg_vars=agg_vars, lag_vars=lag_vars)

cov_methods = c("pearson", "spearman")


@





\subsection{LASSO}

<<lasso, cache=TRUE>>=
lasso_results <- Lasso(DF, var_list,
    num_lambdas=num_lambdas,
    testing_months=testing_months,
    training_months=training_months, verbose=verbose)

@



What do the model errors look like for known locations and future dates?
<<known_error>>=
p1 <- plot_error_curve(lasso_results, "future")
print(p1)
@


What do the model errors look like for known locations and future dates?
<<unknown_error>>=
p1 <- plot_error_curve(lasso_results, "unknown")
print(p1)
@



Plot distribution of error across locations, and plot error distributions varied by window time periods.
<<win_loc_plots, out.width='.49\\textwidth'>>=
plot_error_distribution(lasso_results, split_by="location")
plot_error_distribution(lasso_results, split_by="window")
@



Print out error for each location of unknown test set.
<<lasso_location_error_table, results='asis'>>=
key_vars <- c("location", "datetime_key", "window", "set", "actual")
keep_vars <- c(key_vars, paste0(c("predict", "error"),
    which(lasso_results$best_lambda == lambda_sequence(num_lambdas)))) # index of best lambda
best_errors <- as.data.frame(lasso_results$all_hourly_df[,keep_vars])
names(best_errors) <- c(key_vars, "predict", "error")

# what is the predicted error at each location (for optimal lambda)
loc_error <- best_errors[best_errors$set == "unknown",] %>% group_by(location) %>%
    summarise(avg_actual = mean(actual), avg_predict = mean(predict), rmse=sqrt_mean(error^2))
loc_error <- loc_error[order(loc_error$rmse, decreasing=FALSE),]
print(xtable(loc_error, digits=4), include.rownames=FALSE)
rm(loc_error)
@

<<lasso_window_error_table, results='asis'>>=
# what is the predicted error at each location (for optimal lambda)
window_error <- best_errors[best_errors$set == "unknown",] %>% group_by(window) %>%
    summarise(avg_actual = mean(actual), avg_predict = mean(predict), rmse=sqrt_mean(error^2))
window_error <- window_error[order(window_error$rmse, decreasing=FALSE),]
print(xtable(window_error, digits=4), include.rownames=FALSE)
rm(window_error)
@


Are the errors correlated?
<<error_correlation, out.width='.49\\textwidth'>>=
p1 <- plot_spatial_correlation_Lasso(best_errors, "unknown")
map_df <- unique(DF[,c("Latitude", "Longitude", "location_key")])
map_df$location_key <- factor(map_df$location_key)
names(map_df) <- c("lat", "lon", "location")
map <- get_map(location = c(lon=mean(map_df$lon), lat=mean(map_df$lat)),
    zoom=9, maptype="toner-lite", color="bw", messaging=FALSE)
p2 <- ggmap(map) +
    geom_point(data=map_df,
        aes(x=lon, y=lat, colour=location), size=4) +
    labs(x="Longitude", y="Latitude")

print(p1)
print(p2)

@


How about for the training locations?
<<future_error_correlation, out.width='.49\\textwidth'>>=
p1 <- plot_spatial_correlation_Lasso(best_errors, "future")
print(p1)
print(p2)
@
Correlation structure looks about the same.


Use krigging to interpolate locations
<<lasso_krig>>=
library(geoR)

lat_long_df <- DF %>% group_by(location_key) %>% summarise(lat=mean(Latitude), lon=mean(Longitude))
aday <- best_errors[best_errors$set == "future" &
        best_errors$datetime_key >= 2013010200 &
        best_errors$datetime_key < 2013010300,]
krig_df <- left_join(x=aday, y=lat_long_df, by=c("location"="location_key"))
krig_df <- krig_df[,c("lat","lon","predict")]

# a <- read.table("http://www.stat.ucla.edu/~nchristo/statistics_c173_c273/kriging_11.txt", header=TRUE)
b <- as.geodata(krig_df)
b <- jitterDupCoords(x=b, max=.05)
prediction <- ksline(b, cov.model="exp",
    cov.pars=c(10,3.33), nugget=0,
    locations=c(29.90104,-95.32613))

n <- 25
x <- seq(min(krig_df$lat), max(krig_df$lat), length.out=n)
y <- seq(min(krig_df$lon), max(krig_df$lon), length.out=n)
xv <- rep(x, n)
yv <- rep(y, each=n)
in_mat <- as.matrix(cbind(xv,yv))
plot(in_mat)

q <- ksline(b, cov.model="gaussian",cov.pars=c(2,.1), locations=in_mat)
cbind(q$predict[1:5], q$krige.var[1:5])
image(q, val=q$predict)
points(krig_df)


@


Are the errors correlated with time?
<<time_errors>>=
# aggregate results by day
best_errors$datetime <- parse_date_time(best_errors$datetime_key, "%y%m%d%H")
hour(best_errors$datetime) <- 0
agg_df <- best_errors[best_errors$set == "unknown",] %>% group_by(datetime) %>%
    summarise(rmse = sqrt_mean(error^2))
ggplot(agg_df, aes(x=datetime, y=rmse)) +
    geom_point(colour="grey") +
    geom_line(colour="grey") +
    geom_smooth(se=FALSE, colour="black") +
    stat_smooth(method = "lm") +
    theme_bw()


@
















\newpage
\subsection{GLASSO}
<<glasso, cache=TRUE>>=
glasso_results <- Graphical_Lasso(DF, var_list,
    num_lambdas=num_lambdas,
    testing_months=testing_months,
    training_months=training_months,
    verbose=verbose,
    cov_methods=cov_methods)

@

What do the model errors look like for known locations and future dates?
<<calculate_glasso_error>>=
error_curve <- plot_error_curve(glasso_results)
@
<<show_glasso_error, fig.cap=error_curve$null_label>>=
print(error_curve$p1)
@

Plot distribution of error by sliding window time periods.
<<gl_win_loc_plots, out.width='.49\\textwidth'>>=
plot_error_distribution(glasso_results)
@

Are the errors correlated?
<<gl_error_correlation, out.width='.49\\textwidth'>>=
p1 <- plot_spatial_correlation.Graphical_Lasso(glasso_results)
map_df <- unique(DF[,c("Latitude", "Longitude", "location_key")])
map_df$location_key <- factor(map_df$location_key)
names(map_df) <- c("lat", "lon", "location")
map <- get_map(location = c(lon=mean(map_df$lon), lat=mean(map_df$lat)),
    zoom=9, maptype="toner-lite", color="bw", messaging=FALSE)
p2 <- ggmap(map) +
    geom_point(data=map_df,
        aes(x=lon, y=lat, colour=location), size=4) +
    labs(x="Longitude", y="Latitude")

print(p1)
print(p2)

@


How about for the training locations?
<<gl_future_error_correlation, out.width='.49\\textwidth'>>=
p1 <- plot_spatial_correlation_Glasso(best_errors, "future")
print(p1)
print(p2)
@
Correlation structure looks about the same.



Are the errors correlated with time?
<<gl_time_errors>>=
# aggregate results by day
best_errors$datetime <- parse_date_time(best_errors$datetime_key, "%y%m%d%H")
hour(best_errors$datetime) <- 0
agg_df <- best_errors[best_errors$set == "unknown",] %>% group_by(datetime) %>%
    summarise(rmse = sqrt_mean(error^2))
ggplot(agg_df, aes(x=datetime, y=rmse)) +
    geom_point(colour="grey") +
    geom_line(colour="grey") +
    geom_smooth(se=FALSE, colour="black") +
    stat_smooth(method = "lm") +
    theme_bw()


@






























\newpage
\section{Appendix}
CURRENTLY HIDDEN
<<appendix, results='asis', eval=FALSE>>=
library(Hmisc)
latex(describe(DF, descript='Descriptive Statistics of All Features'), file='')
@


\end{document}