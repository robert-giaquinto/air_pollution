%  SET UP LaTeX DEFAULTS  --------------------------------------
\documentclass{article}
\usepackage[sc]{mathpazo}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{3}
% \usepackage[options]{algorithm2e}
\usepackage{url}
\usepackage{setspace}
\usepackage{relsize}
\usepackage{float}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{booktabs}

\usepackage[authoryear]{natbib}
\usepackage[nottoc]{tocbibind}
\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black]{hyperref}
\hypersetup{
    pdfstartview={XYZ null null 1}}
%\usepackage{breakurl}
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[buttonsize=1em]{animate}
\makeatother
\renewcommand{\bibname}{References}

\begin{document}

\title{Sparse Undirected Graphs for Spatiotemporal Modeling: Summary of Data}
\author{Robert A. Giaquinto}
\maketitle

\begin{abstract}
   Abstract goes here!
\end{abstract}

\tableofcontents

%  Base KnitR code ------------------------------------------------------------------
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

opts_chunk$set(fig.align='center',
    fig.show='hold',
    fig.pos='H',
    message=FALSE,
    warning=FALSE,
    echo=TRUE,
    par=TRUE)
options(width=80, stringsAsFactors=FALSE)

library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(xtable)
library(stringr)
library(readr)
library(lubridate)
@


\section{Executive Summary}
THIS NEEDS TO BE UPDATED!!!


\section{Data}
The data are generated by the \texttt{build\_features.py} file. The data are solely from the \textbf{Houston} area (i.e. training set) and comprise all available features for hourly files from the EPA's air pollution website\footnote{\url{http://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html}} between 2005 and 2015.


<<data, results='hide', cache=TRUE>>=

data_dir <- '/Users/robert/documents/umn/air_pollution/data/'

# read in a few rows of data to determine column types
temp <- read.csv(file=paste0(data_dir, "houston_features.csv"), nrow=1000)

# if column types are logical (from poor guessing of type) change to double
col_types <- sapply(temp, class)
col_types <- ifelse(col_types %in% c('logical', 'numeric'), 'd', str_sub(col_types,1,1))
col_types <- paste(col_types, collapse='')

DF <- read_csv(file=paste0(data_dir, "houston_features.csv"),
    progress=FALSE,
    col_types=col_types)
# remove temporary items
rm(temp, col_types)

# to speed up exploration, only use data from 2010 forward
full_locations <- c(167001034L, 201000024L, 201001034L,
    201001035L, 201001039L, 201001042L, 201001050L, 245000022L, 245001050L)
DF <- as.data.frame(DF[DF$date_key >= 20130000 & DF$location_key %in% full_locations,])
DF <- as.data.frame(DF[DF$pm25 < 150,])
# DF$pm25 <- ifelse(DF$pm25 == 0, NA, DF$pm25)
# DF <- DF[complete.cases(DF),]
DF$County_Code <- NULL
DF$State_Code <- NULL

# Create a few custom variables
DF$am_rushhour <- ifelse(DF$Time_Local %in% c(6,7,8), 1, 0)
DF$pm_rushhour <- ifelse(DF$Time_Local %in% c(18,19,20,21), 1, 0)
DF$rushhour <- ifelse(DF$am_rushhour > 0 | DF$pm_rushhour > 0, 1, 0)
@

<<var_names>>=
# define variables to refer to groups of variables
tar_var <- 'pm25'

key_vars <- c("date_key", "datetime_key", "location_key"
#     , "Date_Local"
    , "Latitude"
    , "Longitude"
#     , "Site_Num"
    , "Time_Local"
)

date_vars <- c("day_of_week_cycle",
    "day_of_week_0", "day_of_week_1", "day_of_week_2",
    "day_of_week_3", "day_of_week_4", "day_of_week_5", "day_of_week_6",
    "day_of_year_cycle", "week_of_year_cycle",
    "month_cycle", "month_1",
    "month_2", "month_3", "month_4", "month_5", "month_6", "month_7",
    "month_8", "month_9", "month_10", "month_11", "month_12",
    "day_of_month_cycle",
    "quarter_cycle", "quarter_1", "quarter_2", "quarter_3", "quarter_4",
    "time_cycle",
    "am_rushhour", "pm_rushhour", "rushhour"
)

lag_root_names <- c("pm25_lag", "wind_direction_sin_lag", "wind_direction_cos_lag",
    # "wind_direction_NE_lag", "wind_direction_SE_lag",
    # "wind_direction_NW_lag", "wind_direction_SW_lag",
    "wind_knots_lag", "temperature_lag",
    "pct_humidity_lag", "dewpoint_lag",
    "ozone_lag", "so2_lag", "co_lag", "no2_lag")

lag_vars <- c(paste0(lag_root_names, "1")
#     , paste0(lag_root_names, "2")
#     , paste0(lag_root_names, "3")
)

agg_vars <- c(paste0(lag_root_names, "1", "_agg1")
#     , paste0(lag_root_names, "2", "_agg1")
#     , paste0(lag_root_names, "3", "_agg1")
)

# keep only the variables used above for modeling
DF <- DF[,c(key_vars, tar_var, date_vars, lag_vars, agg_vars)]

# impute remainder of missings
for (n in names(DF)) {
    if (n %in% c(lag_vars, agg_vars)) {
        DF[is.na(DF[,n]), n] <- mean(DF[,n], na.rm=TRUE)
    }
}
@




There are 270 total features, many have been derived by lagging or lagging and aggregating the basic features. Additionally, there are binary features that capture mean-shifts related to days of the week, months, quarters, and days of the month.

<<feature_names, results='asis'>>=

# print names of the columns
all_names <- names(DF)
uniq_names <- unique(
    str_replace_all(
            str_replace_all(all_names[all_names %in% c(lag_vars, agg_vars, date_vars)],
                '(month_|quarter_|day_of_week_|day_of_month_|Time_Local_)([x0-9]+)', '\\1X'),
            '(lag)([_a-z0-9]+)', '\\1')
    )
if ((length(uniq_names) %% 2) > 0) {
    uniq_names <- c(uniq_names, "")
}
name_df <- data.frame(x1 = uniq_names[1:(length(uniq_names)/2)],
    x2 = uniq_names[(1 + length(uniq_names)/2):length(uniq_names)])
names(name_df) <- c("Features", "Additional Features")
print(xtable(name_df), include.rownames=FALSE, booktabs=TRUE)
@


Finally, the same date features have to converted into a continuous and cyclic number (e.g. Sundays and Mondays are close together)

<<cycles, results='asis'>>=
vars <- c("day_of_week_0",
    "day_of_week_1",
    "day_of_week_2",
    "day_of_week_3",
    "day_of_week_4",
    "day_of_week_5",
    "day_of_week_6",
    "day_of_week_cycle")
example <- as.data.frame(unique(DF[,vars]))
# order results
example <- example[order(example$day_of_week_cycle),]
#print to table
print(xtable(example, digits=0), include.rownames=FALSE, scalebox=.75)
@


<<cleanup_data, echo=FALSE, results='hide'>>=
rm(example, name_df, uniq_names, vars)
@
A full description of all the features can be found in the Appendix.

\subsection{Missing Data}
Missing data were replaced with averages from all other stations at the same point in time (i.e. the same hour). If there was still missing data, then the missing values were replaced with averages from all other stations on that same day. This appoach is reasonable since many of the missing values come from weather related features. It's reasonable to assume that within the same metropolitan area the temperature, wind speed and direction, etc. will be fairly similar. Moreover, missing data is largely the result of not all monitoring stations being equiped with the necessary instruments to capture all weather related features -- thus, missing data is not correlated to certain events that impact the recording of data.


\section{Evalutation}
This is how I split the data into testing and training.

Since the goal is predictive accuracy I evaluate the models based on prediction accuracy on the hold-out data set.


\newpage
\section{Appendix}
CURRENTLY HIDDEN
<<appendix, results='asis'>>=
library(Hmisc)
latex(describe(DF, descript='Descriptive Statistics of All Features'), file='')
@


\end{document}