%  SET UP LaTeX DEFAULTS  --------------------------------------
\documentclass{article}
\usepackage[sc]{mathpazo}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{3}
% \usepackage[options]{algorithm2e}
\usepackage{url}
\usepackage{setspace}
\usepackage{relsize}
\usepackage{float}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{booktabs}

\usepackage[authoryear]{natbib}
\usepackage[nottoc]{tocbibind}
\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black]{hyperref}
\hypersetup{
    pdfstartview={XYZ null null 1}}
%\usepackage{breakurl}
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[buttonsize=1em]{animate}
\makeatother
\renewcommand{\bibname}{References}

\begin{document}

\title{Sparse Undirected Graphs for Spatiotemporal Modeling: Summary of LASSO Performance}
\author{Robert A. Giaquinto}
\maketitle

\begin{abstract}
   Abstract goes here!
\end{abstract}

\tableofcontents

%  Base KnitR code ------------------------------------------------------------------
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

opts_chunk$set(fig.align='center',
    fig.show='hold',
    fig.pos='H',
    message=FALSE,
    warning=FALSE,
    echo=FALSE,
    par=TRUE)
options(width=80, stringsAsFactors=FALSE)

library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(xtable)
library(stringr)
library(readr)
library(glmnet)
library(glmnetUtils)
library(lubridate)
library(RColorBrewer)
library(ggmap)
library(geoR)

source_dir <- '/Users/robert/documents/umn/air_pollution/src/main/'
source(paste0(source_dir, 'Utility_Functions.R'))
source(paste0(source_dir, 'Lasso.R'))
source(paste0(source_dir, 'Data_Prep.R'))
@

<<training_parameters>>=
testing_months=6
training_months=24
verbose=FALSE
num_lambdas <- 25
var_list <- list(tar_var=tar_var, date_vars=date_vars, agg_vars=agg_vars, lag_vars=lag_vars)
@

<<lasso>>=
lasso_results <- Lasso(DF, var_list,
    num_lambdas=num_lambdas,
    testing_months=testing_months,
    training_months=training_months, verbose=verbose)

@

\section{Introduction}
As a baseline model for comparison, the air pollution (as measured by particulate matter less than 2.5 micrometers, denoted PM25) is modeled using regularized linear regression via the LASSO. The LASSO offers a feature select method since non-informative coefficients are driven towards zero, allowing many interactions of features to be tested. In particular, interactions between temporal, and lagged (or lagged moving average) features are considered. The LASSO assumes PM25 follows a guassian distribution.

The LASSO is trained on \Sexpr{training_months} months of data, with predictions and errors records on the following \Sexpr{testing_months} months of data. Then the training and testing dates are shifted forward in time \Sexpr{testing_months} months and the model is re-trained and re-tested. Results are saved for all values of lambdas across all windows -- instead of a traditional cross-validation approach which may find the best lambda (via CV) on a single training window and then move on. The advantages of this approach are that errors can be aggregated across moving windows in order to find a value of lambda which is most suitable for any point in time.


\section{Error as a Function of Model Complexity}
The primary means of evaluation of the LASSO is the root mean squared error (RMSE) calculated over all prediction windows by each lambda value. The lambda value is the parameter which governs model complexity in the LASSO. As shown in (EQUATION FOR LASSO), model complexity is measured by the absolute value of the coefficients. The plot below shows how RMSE varys by lambda. Larger values of lambda equate to larger penalties for complexity. As lambda increases the model approaches the null model in which predictions are simply the average PM25 seen in the training set. At the other extreme, when lambda is zero, the model is equivalent to linear regression. The plot below shows that a $lambda=$\Sexpr{round(lasso_results$best_lambda, 3)} results in the best predicted error on the test set of future data. Moreover, the model performs well, showing a drop in RMSE over the null model by \Sexpr{round(100*(1 - lasso_results$best_rmse/lasso_results$null_rmse))}\%
<<known_error>>=
p1 <- plot_error_curve(lasso_results)
print(p1)
@





\section{Distribution of Error Across Time and Space}
In addition to looking at the overall performance, it's helpful to examine how RMSE varies by time conditional on location, and location conditioned on time. In the case of time conditioned on location, this plot helps
<<unpooled_window>>=
error_distribution <- plot_error_distribution(lasso_results, conditional="window")
@
<<unpooled_window_plots, fig.cap=error_distribution$plot_caption>>=
print(error_distribution$out_plot)
@

Plot distribution of error by sliding window time periods.
<<unpooled_location>>=
error_distribution <- plot_error_distribution(lasso_results, conditional="location_key")
@
<<unpooled_location_plots, fig.cap=error_distribution$plot_caption>>=
print(error_distribution$out_plot)
@

On a higher level, the RMSE for each particular is shown below:
<<lasso_window_error_table, results='asis'>>=
key_vars <- c("location_key", "datetime_key", "window", "actual")
keep_vars <- c(key_vars, paste0(c("predict", "error"),
    which(lasso_results$best_lambda == lambda_sequence(num_lambdas)))) # index of best lambda
best_errors <- as.data.frame(lasso_results$all_hourly_df[,keep_vars])
names(best_errors) <- c(key_vars, "predict", "error")

# what is the predicted error at each window (for optimal lambda)
window_error <- best_errors %>% group_by(window) %>%
    summarise(avg_actual = mean(actual), avg_predict = mean(predict), RMSE=rmse(error))
window_error <- window_error[order(window_error$RMSE, decreasing=FALSE),]
print(xtable(window_error, digits=4), include.rownames=FALSE)
rm(window_error)
@

And similarly for locations:
<<lasso_location_error_table, results='asis'>>=
# what is the predicted error at each window (for optimal lambda)
location_error <- best_errors %>% group_by(location_key) %>%
    summarise(avg_actual = mean(actual), avg_predict = mean(predict), RMSE=rmse(error))
location_error <- location_error[order(location_error$RMSE, decreasing=FALSE),]
print(xtable(location_error, digits=4), include.rownames=FALSE)
rm(location_error)
@



\section{Spatial Correlation}
Are the errors correlated?
<<error_correlation, out.width='.49\\textwidth'>>=
spatial_plot <- plot_spatial_correlation(lasso_results)
map_df <- unique(DF[,c("Latitude", "Longitude", "location_key")])
map_df$location_key <- factor(map_df$location_key)
names(map_df) <- c("lat", "lon", "location")
map <- get_map(location = c(lon=mean(map_df$lon), lat=mean(map_df$lat)),
    zoom=9, maptype="toner-lite", color="bw", messaging=FALSE)
map_plot <- ggmap(map) +
    geom_point(data=map_df,
        aes(x=lon, y=lat, colour=location), size=4) +
    labs(x="Longitude", y="Latitude")

print(spatial_plot)
print(map_plot)

@




\end{document}